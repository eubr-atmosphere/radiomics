{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\r\n",
      "# -*- coding: utf-8 -*-\r\n",
      "\"\"\"\r\n",
      "Created on Thu Jan 31 11:08:01 2019\r\n",
      "\r\n",
      "@author: ajp\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "from keras.models import Model\r\n",
      "from keras.layers import Dense, Activation, Flatten, Input\r\n",
      "from keras.layers import Conv2D\r\n",
      "from keras.layers import MaxPooling2D\r\n",
      "from keras.optimizers import Adadelta\r\n",
      "from keras import backend as K\r\n",
      "from keras.callbacks import ModelCheckpoint, Callback\r\n",
      "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import cv2\r\n",
      "import os\r\n",
      "\r\n",
      "import tensorflow as tf\r\n",
      "import math\r\n",
      "import horovod.keras as hvd\r\n",
      "\r\n",
      "\r\n",
      "def featurewise_normalization(images):\r\n",
      "    mean = np.mean(images)\r\n",
      "    std = np.std(images)\r\n",
      "    np.save(os.path.join(paths['main_path'], 'training','mean.npy'),mean)\r\n",
      "    np.save(os.path.join(paths['main_path'], 'training','std.npy'),std)\r\n",
      "    images = np.subtract(images,mean)\r\n",
      "    images = np.divide(images,std)\r\n",
      "    return images\r\n",
      "\r\n",
      "def samplewise_intensity_normalization(images):\r\n",
      "    for i in range(images.shape[0]):\r\n",
      "        img = images[i,:,:]\r\n",
      "        maxim = np.max(img)\r\n",
      "        minim = np.min(img)\r\n",
      "        images[i,:,:] = (img - minim) / (maxim - minim)\r\n",
      "    return images\r\n",
      "\r\n",
      "def augmentation_rotation(image,augmentation_params):\r\n",
      "    image_rot = np.copy(image)\r\n",
      "    \r\n",
      "    if(flip(augmentation_params['rotation_prob']) == 'augment'):   \r\n",
      "        rows,cols, channels = image.shape\r\n",
      "        angle = np.random.uniform(augmentation_params['min_angle'], augmentation_params['max_angle'])\r\n",
      "        M = cv2.getRotationMatrix2D((cols/2,rows/2),angle,1)\r\n",
      "        image_rot = cv2.warpAffine(image,M,(cols,rows))[..., np.newaxis]\r\n",
      "\r\n",
      "    return image_rot\r\n",
      "  \r\n",
      "\r\n",
      "def flip(p):\r\n",
      "    return 'augment' if np.random.random() < p else 'pass'\r\n",
      "\r\n",
      "def training_data_augmentation(img, augmentation_params):\r\n",
      "        \r\n",
      "    img_ = augmentation_rotation(img, augmentation_params)\r\n",
      "    \r\n",
      "    img_ = img_.astype('float32')\r\n",
      "    \r\n",
      "    return img_\r\n",
      "\r\n",
      "def data_generator(X_train, y_train, augmentation_params, batch_size):\r\n",
      "    \r\n",
      "    batch_images = np.zeros([batch_size, X_train.shape[1], X_train.shape[2], X_train.shape[3]])\r\n",
      "    batch_labels = np.zeros([batch_size,  y_train.shape[1]])\r\n",
      "    index = 0\r\n",
      "    while True:\r\n",
      "        for b in range(batch_size):\r\n",
      "            if index == X_train.shape[0]:\r\n",
      "                index = 0\r\n",
      "                shuffle_indexes = np.arange(X_train.shape[0])\r\n",
      "                np.random.shuffle(shuffle_indexes)\r\n",
      "                X_train = X_train[shuffle_indexes]\r\n",
      "                y_train = y_train[shuffle_indexes]\r\n",
      "                \r\n",
      "            img = training_data_augmentation(X_train[index], augmentation_params)\r\n",
      "            \r\n",
      "            batch_images[b] = img\r\n",
      "            batch_labels[b] = y_train[index]\r\n",
      "            \r\n",
      "            index += 1\r\n",
      "        yield batch_images, batch_labels\r\n",
      "\r\n",
      "def classifier():\r\n",
      "    \r\n",
      "    inputs = Input((image_params['img_rows'],image_params['img_cols'], image_params['img_channels']))\r\n",
      "    \r\n",
      "    conv1 = Conv2D(32, (3, 3), padding='same')(inputs)\r\n",
      "    conv1 = Activation('relu')(conv1)\r\n",
      "    pool1 = MaxPooling2D(pool_size=(2, 2), strides = (2,2))(conv1)\r\n",
      "    \r\n",
      "    conv2 = Conv2D(64, (3, 3), padding='same')(pool1)\r\n",
      "    conv2 = Activation('relu')(conv2)\r\n",
      "    pool2 = MaxPooling2D(pool_size=(2, 2), strides = (2,2))(conv2)\r\n",
      "    \r\n",
      "    conv3 = Conv2D(128, (3, 3), padding='same')(pool2)\r\n",
      "    conv3 = Activation('relu')(conv3)\r\n",
      "    pool3 = MaxPooling2D(pool_size=(2, 2), strides = (2,2))(conv3)\r\n",
      "    \r\n",
      "    conv4 = Conv2D(256, (3, 3), padding='same')(pool3)\r\n",
      "    conv4 = Activation('relu')(conv4)\r\n",
      "    pool4 = MaxPooling2D(pool_size=(2, 2), strides = (2,2))(conv4)\r\n",
      "    \r\n",
      "    conv5 = Conv2D(512, (3, 3), padding='same')(pool4)\r\n",
      "    conv5 = Activation('relu')(conv5)\r\n",
      "    pool5 = MaxPooling2D(pool_size=(2, 2), strides = (2,2))(conv5)\r\n",
      "    \r\n",
      "    output = Flatten()(pool5)\r\n",
      "    output = Dense(image_params['n_labels'])(output)\r\n",
      "    output = Activation('softmax')(output)\r\n",
      "    \r\n",
      "    model = Model(inputs=[inputs], outputs=[output])\r\n",
      "         \r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "def write_params_txt(paths, image_params, augmentation_params, training_params, comments):\r\n",
      "    \r\n",
      "    experiment = paths['model_save'].split('/')[len(paths['model_save'].split('/')) - 1]\r\n",
      "    file = open(paths['model_save'] + '/Params.txt', 'w')\r\n",
      "    file.write(experiment + '\\n')\r\n",
      "    file.write('\\n')\r\n",
      "    file.write('Preprocess_params \\n')\r\n",
      "    for key in image_params.keys():\r\n",
      "        file.write(key + ': ' + str(image_params[key]) + '\\n')\r\n",
      "    file.write('\\n')\r\n",
      "    file.write('Augmentation_params \\n')\r\n",
      "    for key in augmentation_params.keys():\r\n",
      "        file.write(key + ': ' + str(augmentation_params[key]) + '\\n')\r\n",
      "    file.write('\\n')\r\n",
      "    file.write('Training_params \\n')\r\n",
      "    for key in training_params.keys():\r\n",
      "        file.write(key + ': ' + str(training_params[key]) + '\\n')\r\n",
      "    file.write('\\n')\r\n",
      "    file.write('Comments \\n')\r\n",
      "    for key in comments.keys():\r\n",
      "        file.write(key + ': ' + str(comments[key]) + '\\n')\r\n",
      "    file.close()\r\n",
      "\r\n",
      "# Class to extract the loss function after each epoch in losses.txt\r\n",
      "class LossHistory(Callback):\r\n",
      "    \r\n",
      "    def __init__(self, model_save_path):\r\n",
      "        self.model_save_path = model_save_path\r\n",
      "        self.epoch_count = 0\r\n",
      "    def on_train_begin(self, logs={}):\r\n",
      "        self.losses = []    \r\n",
      "        # Erase the content if there is a previous losses file\r\n",
      "        self.f = open(self.model_save_path + '/losses.txt', 'w')\r\n",
      "        self.f.close()\r\n",
      "        \r\n",
      "    def on_epoch_end(self, batch, logs={}):\r\n",
      "        self.epoch_count = self.epoch_count + 1\r\n",
      "        self.losses.append(logs.get('loss'))\r\n",
      "        # Open the empty file to append new information\r\n",
      "        self.f = open(self.model_save_path + '/losses.txt', 'a')\r\n",
      "        self.f.write('Epoch ' + str(self.epoch_count) + ' ' + str(logs) + '\\n')\r\n",
      "        self.f.close()  \r\n",
      "    \r\n",
      "def train():\r\n",
      "    \r\n",
      "            \r\n",
      "    print('-'*30)\r\n",
      "    print('Loading train data...')\r\n",
      "    print('-'*30)\r\n",
      "    \r\n",
      "    x_train = np.load(os.path.join(paths['main_path'], 'RHD_data' , 'x_train.npy'))\r\n",
      "    print('x_train loaded')\r\n",
      "    y_train = np.load(os.path.join(paths['main_path'], 'RHD_data', 'y_train.npy'))\r\n",
      "    print('y_train loaded')\r\n",
      "    x_val = np.load(os.path.join(paths['main_path'], 'RHD_data', 'x_val.npy'))\r\n",
      "    print('x_val loaded')\r\n",
      "    y_val = np.load(os.path.join(paths['main_path'], 'RHD_data', 'y_val.npy'))\r\n",
      "    print('y_val loaded')\r\n",
      "    \r\n",
      "    indexes = np.arange(x_train.shape[0])\r\n",
      "    np.random.shuffle(indexes)\r\n",
      "    \r\n",
      "    x_train = x_train[indexes]\r\n",
      "    y_train = y_train[indexes]\r\n",
      "    \r\n",
      "    indexes = np.arange(x_val.shape[0])\r\n",
      "    np.random.shuffle(indexes)\r\n",
      "    \r\n",
      "    x_val = x_val[indexes]\r\n",
      "    y_val = y_val[indexes]\r\n",
      "    \r\n",
      "    \r\n",
      "    if image_params['samplewise_intensity_normalization'] == True:\r\n",
      "        x_train = samplewise_intensity_normalization(x_train)\r\n",
      "        x_val = samplewise_intensity_normalization(x_val)\r\n",
      "    \r\n",
      "    if image_params['featurewise_normalization'] == True:\r\n",
      "        x_train = featurewise_normalization(x_train)\r\n",
      "        x_val = featurewise_normalization(x_val)   \r\n",
      "        \r\n",
      "    x_train = x_train[..., np.newaxis]\r\n",
      "    x_val = x_val[..., np.newaxis]\r\n",
      "    \r\n",
      "    if not os.path.isdir('training'):\r\n",
      "        os.mkdir('training')\r\n",
      "    \r\n",
      "    print('-'*30)\r\n",
      "    print('Creating and compiling model...')\r\n",
      "    print('-'*30)\r\n",
      "    \r\n",
      "    # Horovod: initialize Horovod.\r\n",
      "    hvd.init()\r\n",
      "    \r\n",
      "    #Horovod: pin GPU to be used to process local rank (one GPU per process)\r\n",
      "    config = tf.ConfigProto()\r\n",
      "    config.gpu_options.allow_growth = True\r\n",
      "    config.gpu_options.visible_device_list = str(hvd.local_rank())\r\n",
      "    K.set_session(tf.Session(config=config))\r\n",
      "    \r\n",
      "    # Horovod: adjust number of epochs based on number of GPUs.\r\n",
      "    epochs = int(math.ceil(12.0 / hvd.size()))\r\n",
      "    \r\n",
      "    # Horovod: adjust learning rate based on number of GPUs.\r\n",
      "    opt = Adadelta(1.0 * hvd.size())\r\n",
      "    \r\n",
      "    # Horovod: add Horovod Distributed Optimizer.\r\n",
      "    opt = hvd.DistributedOptimizer(opt)\r\n",
      "          \r\n",
      "    data_generator_train = data_generator(x_train, y_train, augmentation_params, training_params['batch_size'])\r\n",
      "    \r\n",
      "    print('-'*30)\r\n",
      "    print('Fitting model...')\r\n",
      "    print('-'*30)\r\n",
      "    model = classifier()\r\n",
      "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
      "    \r\n",
      "    callbacks = [\r\n",
      "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\r\n",
      "    # This is necessary to ensure consistent initialization of all workers when\r\n",
      "    # training is started with random weights or restored from a checkpoint.\r\n",
      "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\r\n",
      "    ]   \r\n",
      "    \r\n",
      "    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\r\n",
      "    if hvd.rank() == 0:\r\n",
      "        callbacks.append(ModelCheckpoint(filepath=(os.path.join(paths['main_path'], 'training', 'classifier.hdf5')), monitor='val_loss', save_best_only=True))\r\n",
      "\r\n",
      "\r\n",
      "    model.fit_generator(generator = data_generator_train, \r\n",
      "                        steps_per_epoch = x_train.shape[0]//training_params['batch_size'],\r\n",
      "                        epochs = epochs,\r\n",
      "                        verbose=1,\r\n",
      "                        callbacks=callbacks,\r\n",
      "                        validation_data = (x_val, y_val))\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    \r\n",
      "    paths = {\r\n",
      "            'main_path' : '/mnt/share/vol001/classify_views'\r\n",
      "            }\r\n",
      "\r\n",
      "    image_params = {\r\n",
      "                    'img_rows': 320, \r\n",
      "                    'img_cols': 240,\r\n",
      "                    'img_channels': 1,\r\n",
      "                    'n_labels': 3,\r\n",
      "                    'featurewise_normalization': True,\r\n",
      "                    'samplewise_intensity_normalization': True\r\n",
      "                    }\r\n",
      "    \r\n",
      "    augmentation_params = {\r\n",
      "                            'rotation': True,\r\n",
      "                            'min_noise_var':0.001,\r\n",
      "                            'max_noise_var':0.01,\r\n",
      "                            'min_angle': -5,\r\n",
      "                            'max_angle': 5,\r\n",
      "                            'rotation_prob': 0.3\r\n",
      "                        }\r\n",
      "    \r\n",
      "    training_params = {\r\n",
      "                        'val_fraction':0.2,\r\n",
      "                        'architecture': 'classifier',\r\n",
      "                        'batch_size': 40,\r\n",
      "                        }\r\n",
      "    \r\n",
      "    train()\r\n"
     ]
    }
   ],
   "source": [
    "!cat /mnt/share/vol001/classify_views/classify_views.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh not successful for host clusterworker-1:\r\n",
      "Connection closed by 10.243.5.53 port 22\r\n",
      "\r\n",
      "ssh not successful for host clusterworker-0:\r\n",
      "Connection closed by 10.243.4.58 port 22\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "!LD_LIBRARY_PATH=/usr/lib/:/usr/local/lib/ \\\n",
    "HOROVOD_HIERARCHICAL_ALLREDUCE=1 \\\n",
    "HOROVOD_FUSION_THRESHOLD=134217728 \\\n",
    " /opt/conda/bin/horovodrun -np 2 \\\n",
    "                           -H clusterworker-0:1,clusterworker-1:1 \\\n",
    "                           python3 /mnt/share/vol001/classify_views/classify_views.py\n",
    "end = time.time()\n",
    "print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
